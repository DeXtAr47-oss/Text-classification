# AI vs Human Text Classifier

This project aims to classify whether a given text is written by a human or generated by an AI model.
It combines traditional NLP preprocessing, feature extraction using BERT embeddings, and machine learning models (Naive Bayes, Logistic Regression, Random Forest, KNN, XGBoost) to build a robust classification pipeline.

# Workflow Overview
## 1. Data Loading
Two datasets are loaded:

labelled_train_set.csv → contains labeled texts (“Human-written” or “AI-generated”)

unlabelled_test2.csv → contains texts without labels for testing

The dataset are uploaded to google drive ([click here](https://drive.google.com/drive/folders/148T2B8jfCIztRq0I7-rfSZVXJ46Bf2q5?usp=drive_link)).

## 2. Preprocessing

Removes:

Duplicate and null entries

Emojis, punctuation, numbers, and special symbols

Tokenization using nltk.word_tokenize

Stopword removal using nltk.corpus.stopwords

Lemmatization using WordNetLemmatizer

## 3. Exploratory Data Analysis

## Class distribution visualization using matplotlib.pie.
![class distribution](https://github.com/DeXtAr47-oss/Text-classification/blob/8581a26cef1b03e51c26e56ab1e199d5ba4fc2ad/images/class_distribution.png)
    
## Basic statistics like number of characters, words, and sentences.
* Character distribution
    ![character distribution](https://github.com/DeXtAr47-oss/Text-classification/blob/8581a26cef1b03e51c26e56ab1e199d5ba4fc2ad/images/character_distribution.png)
  
* Total no. of words distribution
    ![words distribution](https://github.com/DeXtAr47-oss/Text-classification/blob/8581a26cef1b03e51c26e56ab1e199d5ba4fc2ad/images/word_distribution.png)

* Total no. of sentences distribution
    ![sentence distribution](https://github.com/DeXtAr47-oss/Text-classification/blob/8581a26cef1b03e51c26e56ab1e199d5ba4fc2ad/images/sentence_distribution.png)

* Most frequent words
    ![most frequent words](https://github.com/DeXtAr47-oss/Text-classification/blob/8581a26cef1b03e51c26e56ab1e199d5ba4fc2ad/images/most_frequent_words.png)

## 4. Feature Engineering

Tfidf Embeddings: Converts text into a fixed-length numverical vector.
    ```
        tf = TfidfVectorizer(max_features=4000)
    ```

BERT Embeddings: Converts each text into a fixed-length numerical vector using a pre-trained BERT model.

    bert_X_train = df["Article"].apply(get_embeddings)
    bert_X_test  = df1["Article"].apply(get_embeddings)

Embeddings are standardized using StandardScaler.

## 5. Model Training

Trained several classification models using train_test_split:

Gaussian Naive Bayes

Bernoulli Naive Bayes (GridSearchCV tuned)

Logistic Regression (GridSearchCV tuned)

Random Forest Classifier (GridSearchCV tuned)

K-Nearest Neighbors (GridSearchCV tuned)

XGBoost Classifier (GridSearchCV tuned)

Each model is optimized with GridSearchCV to find the best hyperparameters.

## 6. Model Evaluation

For every model, classification metrics are reported:

Accuracy

Precision

Recall

F1-Score

Confusion Matrix
