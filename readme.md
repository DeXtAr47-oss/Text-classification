# AI vs Human Text Classifier

This project aims to classify whether a given text is written by a human or generated by an AI model.
It combines traditional NLP preprocessing, feature extraction using BERT embeddings, and machine learning models (Naive Bayes, Logistic Regression, Random Forest, KNN, XGBoost) to build a robust classification pipeline.

# Workflow Overview
## 1. Data Loading
Two datasets are loaded:

labelled_train_set.csv → contains labeled texts (“Human-written” or “AI-generated”)

unlabelled_test2.csv → contains texts without labels for testing

The dataset are uploaded to google drive ([click here](https://drive.google.com/drive/folders/148T2B8jfCIztRq0I7-rfSZVXJ46Bf2q5?usp=drive_link)).

## 2. Preprocessing

Removes:

Duplicate and null entries

Emojis, punctuation, numbers, and special symbols

Tokenization using nltk.word_tokenize

Stopword removal using nltk.corpus.stopwords

Lemmatization using WordNetLemmatizer

## 3. Exploratory Data Analysis

## Class distribution visualization using matplotlib.pie.
![class distribution](https://github.com/DeXtAr47-oss/Text-classification/blob/8581a26cef1b03e51c26e56ab1e199d5ba4fc2ad/images/class_distribution.png)
    
## Basic statistics like number of characters, words, and sentences.
* Character distribution
    ![character distribution](https://github.com/DeXtAr47-oss/Text-classification/blob/8581a26cef1b03e51c26e56ab1e199d5ba4fc2ad/images/character_distribution.png)
  
* Total no. of words distribution
    ![words distribution](https://github.com/DeXtAr47-oss/Text-classification/blob/8581a26cef1b03e51c26e56ab1e199d5ba4fc2ad/images/word_distribution.png)

* Total no. of sentences distribution
    ![sentence distribution](https://github.com/DeXtAr47-oss/Text-classification/blob/8581a26cef1b03e51c26e56ab1e199d5ba4fc2ad/images/sentence_distribution.png)

* Most frequent words
    ![most frequent words](https://github.com/DeXtAr47-oss/Text-classification/blob/8581a26cef1b03e51c26e56ab1e199d5ba4fc2ad/images/most_frequent_words.png)

## 4. Feature Engineering

Tfidf Embeddings: Converts text into a fixed-length numverical vector.
    ```
        tf = TfidfVectorizer(max_features=4000)
    ```

BERT Embeddings: Converts each text into a fixed-length numerical vector using a pre-trained BERT model.

    bert_X_train = df["Article"].apply(get_embeddings)
    bert_X_test  = df1["Article"].apply(get_embeddings)

Embeddings are standardized using StandardScaler.

## 5. Model Training

Trained several classification models using train_test_split:

Gaussian Naive Bayes

Bernoulli Naive Bayes (GridSearchCV tuned)

Logistic Regression (GridSearchCV tuned)

Random Forest Classifier (GridSearchCV tuned)

K-Nearest Neighbors (GridSearchCV tuned)

XGBoost Classifier (GridSearchCV tuned)

Each model is optimized with GridSearchCV to find the best hyperparameters.

## 6. Model Evaluation

### TF-IDF Vectorization Results

| Model | Accuracy | Class | Precision | Recall | F1-Score | Support |
|-------|----------|-------|-----------|--------|----------|---------|
| **Gaussian Naive Bayes** | 0.80 | 0 | 0.81 | 0.99 | 0.89 | 80 |
| | | 1 | 0.50 | 0.05 | 0.09 | 20 |
| **Bernoulli Naive Bayes** (GridSearchCV) | 0.84 | 0 | 0.90 | 0.90 | 0.90 | 80 |
| | | 1 | 0.60 | 0.60 | 0.60 | 20 |
| **Logistic Regression** (GridSearchCV) | 0.82 | 0 | 0.82 | 0.99 | 0.90 | 80 |
| | | 1 | 0.75 | 0.15 | 0.25 | 20 |
| **Random Forest** (GridSearchCV) | 0.84 | 0 | 0.86 | 0.96 | 0.91 | 80 |
| | | 1 | 0.70 | 0.35 | 0.47 | 20 |
| **K-Nearest Neighbors** (GridSearchCV) | 0.80 | 0 | 0.80 | 1.00 | 0.89 | 80 |
| | | 1 | 0.00 | 0.00 | 0.00 | 20 |
| **XGBoost** (GridSearchCV) | 0.83 | 0 | 0.89 | 0.90 | 0.89 | 80 |
| | | 1 | 0.58 | 0.55 | 0.56 | 20 |

### Summary

| Model | Overall Accuracy |
|-------|------------------|
| Bernoulli Naive Bayes | **0.84** |
| Random Forest | **0.84** |
| XGBoost | 0.83 |
| Logistic Regression | 0.82 |
| Gaussian Naive Bayes | 0.80 |
| K-Nearest Neighbors | 0.80 |

### BERT Embeddings Results

| Model | Accuracy | Class | Precision | Recall | F1-Score | Support |
|-------|----------|-------|-----------|--------|----------|---------|
| **Gaussian Naive Bayes** | 0.83 | 0 | 0.94 | 0.84 | 0.89 | 80 |
| | | 1 | 0.55 | 0.80 | 0.65 | 20 |
| **Bernoulli Naive Bayes** | 0.89 | 0 | 0.91 | 0.96 | 0.93 | 80 |
| | | 1 | 0.80 | 0.60 | 0.69 | 20 |
| **Logistic Regression** | 0.91 | 0 | 0.94 | 0.95 | 0.94 | 80 |
| | | 1 | 0.79 | 0.75 | 0.77 | 20 |
| **Random Forest** | 0.88 | 0 | 0.88 | 0.99 | 0.93 | 80 |
| | | 1 | 0.90 | 0.45 | 0.60 | 20 |
| **K-Nearest Neighbors** | 0.87 | 0 | 0.88 | 0.97 | 0.92 | 80 |
| | | 1 | 0.82 | 0.45 | 0.58 | 20 |
| **XGBoost** | 0.91 | 0 | 0.91 | 0.99 | 0.95 | 80 |
| | | 1 | 0.92 | 0.60 | 0.73 | 20 |

### Summary

| Model | Overall Accuracy |
|-------|------------------|
| Logistic Regression | **0.91** |
| XGBoost | **0.91** |
| Bernoulli Naive Bayes | 0.89 |
| Random Forest | 0.88 |
| K-Nearest Neighbors | 0.87 |
| Gaussian Naive Bayes | 0.83 |

**Best Performers:** Logistic Regression and XGBoost both achieved 91% accuracy with BERT embeddings.

**Key Observations:**
- BERT embeddings significantly improved performance across all models compared to TF-IDF
- Average accuracy improvement: ~7% across all models
- Class 1 (minority class) predictions improved substantially with BERT
